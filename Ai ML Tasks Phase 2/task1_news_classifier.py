# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15A8R9qqJJSRiEB98zv7mDBg7q5ceIOlK
"""

# ===============================
# 1Ô∏è‚É£ Install Libraries (with update fix)
# ===============================
!pip install -q --upgrade pip
!pip install -q transformers datasets torch scikit-learn accelerate

# ===============================
# 2Ô∏è‚É£ Imports
# ===============================
import torch
from datasets import load_dataset
from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer
from sklearn.metrics import accuracy_score, f1_score
import numpy as np

# ===============================
# 3Ô∏è‚É£ GPU Check
# ===============================
print("GPU Available:", torch.cuda.is_available())

# ===============================
# 4Ô∏è‚É£ Load Dataset (AG News)
# ===============================
dataset = load_dataset("ag_news")
print(dataset)

# ===============================
# 5Ô∏è‚É£ Load Tokenizer
# ===============================
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# ===============================
# 6Ô∏è‚É£ Tokenization Function
# ===============================
def tokenize_data(example):
    return tokenizer(
        example["text"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

tokenized_dataset = dataset.map(tokenize_data, batched=True)

# ===============================
# 7Ô∏è‚É£ Safe Column Handling
# ===============================
if "text" in tokenized_dataset["train"].column_names:
    tokenized_dataset = tokenized_dataset.remove_columns(["text"])

if "label" in tokenized_dataset["train"].column_names:
    tokenized_dataset = tokenized_dataset.rename_column("label", "labels")

tokenized_dataset.set_format("torch")
print("Columns after formatting:", tokenized_dataset["train"].column_names)

# ===============================
# 8Ô∏è‚É£ Load BERT Model
# ===============================
model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=4
)

# ===============================
# 9Ô∏è‚É£ Training Arguments (Colab GPU Friendly)
# ===============================
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="no",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=2,
    weight_decay=0.01,
    logging_steps=200,
    report_to="none"
)

# ===============================
# üîü Metrics Function
# ===============================
def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds, average="weighted")
    }

# ===============================
# 1Ô∏è‚É£1Ô∏è‚É£ Trainer
# ===============================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    compute_metrics=compute_metrics
)

# ===============================
# 1Ô∏è‚É£2Ô∏è‚É£ Train Model
# ===============================
trainer.train()

# ===============================
# 1Ô∏è‚É£3Ô∏è‚É£ Evaluate Model
# ===============================
eval_result = trainer.evaluate()
print("Evaluation Results:", eval_result)

# ===============================
# 1Ô∏è‚É£4Ô∏è‚É£ Quick Prediction Test
# ===============================
text = "NASA launches new satellite into space"

inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
outputs = model(**inputs)

prediction = outputs.logits.argmax(dim=1).item()
labels = ["World", "Sports", "Business", "Sci/Tech"]
print("Prediction:", labels[prediction])