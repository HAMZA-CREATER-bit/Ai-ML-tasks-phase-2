# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15A8R9qqJJSRiEB98zv7mDBg7q5ceIOlK
"""

# ===============================
# 1Ô∏è‚É£ Install Required Libraries
# ===============================
!pip install -q streamlit langchain langchain-embeddings langchain-community faiss-cpu transformers

# ===============================
# 2Ô∏è‚É£ Imports
# ===============================
import os
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
# from langchain_deepseek import ChatDeepSeek  # Optional if you have DeepSeek

# ===============================
# 3Ô∏è‚É£ Load and split PDF into chunks
# ===============================
def load_and_split_pdf(pdf_path, chunk_size=500, chunk_overlap=50):
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = splitter.split_documents(docs)
    return chunks

# ===============================
# 4Ô∏è‚É£ HuggingFace embeddings
# ===============================
def get_embeddings():
    embeddings = HuggingFaceEmbeddings(
        model_name="Qwen/Qwen3-Embedding-0.6B",
        model_kwargs={"device": "cpu"}  # change to "cuda" if GPU available
    )
    return embeddings

# ===============================
# 5Ô∏è‚É£ FAISS vector DB
# ===============================
def get_vectordb(chunks, embeddings, index_path="faiss_db"):
    if os.path.exists(os.path.join(index_path, "index.faiss")):
        vectordb = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)
    else:
        vectordb = FAISS.from_documents(chunks, embeddings)
        vectordb.save_local(index_path)
    return vectordb

# ===============================
# 6Ô∏è‚É£ Retrieval QA Chain (HuggingFace LLM)
# ===============================
def create_qa_chain(vectordb):
    retriever = vectordb.as_retriever()

    # Example: HuggingFace pipeline model for demo (replace with your preferred LLM)
    from langchain.llms import HuggingFacePipeline
    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

    model_name = "TheBloke/guanaco-7B-HF"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
    llm = HuggingFacePipeline(pipeline=pipe)

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff"
    )
    return qa_chain

# ===============================
# 7Ô∏è‚É£ Streamlit App
# ===============================
st.title("üìÑ PDF Q&A Chat")
st.write("Upload your PDF and ask questions interactively.")

uploaded_file = st.file_uploader("Choose a PDF file", type="pdf")
if uploaded_file is not None:
    with open("temp.pdf", "wb") as f:
        f.write(uploaded_file.read())

    st.success("PDF uploaded successfully! Processing...")

    chunks = load_and_split_pdf("temp.pdf")
    embeddings = get_embeddings()
    vectordb = get_vectordb(chunks, embeddings)
    qa_chain = create_qa_chain(vectordb)

    query = st.text_input("Ask a question:")
    if query:
        with st.spinner("Generating answer..."):
            try:
                answer = qa_chain.invoke({"query": query})
                st.markdown(f"**Answer:** {answer}")
            except Exception as e:
                st.error(f"Error: {e}")